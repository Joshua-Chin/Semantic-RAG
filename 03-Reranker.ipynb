{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b74260-3bb3-4e2c-a671-44bb0c0c4eff",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0956bb86-2d27-465d-a9a9-76dda1947b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib.load import extract_data, load_benchmark_corpus\n",
    "\n",
    "\n",
    "extract_data()\n",
    "benchmark, corpus = load_benchmark_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bd70f-886f-4fbf-beb6-1e7015c75f21",
   "metadata": {},
   "source": [
    "## Split Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2929d1d0-3cbd-4410-b398-153f3496f312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source_file': 'Fiverr.txt', 'start_index': 2}, page_content='At Fiverr we care about your privacy.\\nWe do not sell or rent your personal information to third parties for their direct marketing purposes without your explicit consent.'),\n",
       " Document(metadata={'source_file': 'Fiverr.txt', 'start_index': 173}, page_content='We do not disclose it to others except as disclosed in this Policy or required to provide you with the services of the Site and mobile applications, meaning - to allow you to buy, sell, share the information you want to share on the Site; to contribute on the forum; pay for products; post reviews and so on; or where we have a legal obligation to do so.'),\n",
       " Document(metadata={'source_file': 'Fiverr.txt', 'start_index': 530}, page_content='We collect information that you provide us or voluntarily share with other users, and also some general technical information that is automatically gathered by our systems, such as IP address, browser information and cookies to enable you to have a better user experience and a more personalized browsing experience.\\n  We will not share information that you provide us in the process of the registration - including your contact information - except as described in this Policy.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n', '!', '?', '.', ':', ';', ',', ' ', ''],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "names, texts = zip(*corpus.items())\n",
    "metadatas = [\n",
    "    {\"source_file\": name}\n",
    "    for idx, name in enumerate(names)\n",
    "]\n",
    "\n",
    "documents = text_splitter.create_documents(corpus.values(), metadatas=metadatas)\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567497ed-7916-4097-9d41-903e5e4ee0f5",
   "metadata": {},
   "source": [
    "## Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9b6f5-db2f-4868-a294-70ac015f9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def compute_similarities(benchmark, documents):\n",
    "    # Load model\n",
    "    model = SentenceTransformer(\n",
    "        \"Qwen/Qwen3-Embedding-8B\",\n",
    "        model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)}\n",
    "    )\n",
    "    # Compute embeddings\n",
    "    document_embeddings = model.encode(\n",
    "        [f\"{document.metadata[\"source_file\"]}: {document.page_content}\" for document in documents],\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    query_embeddings = model.encode(\n",
    "        [test['query'] for test in benchmark],\n",
    "        prompt_name=\"query\",\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    # Compute similarity\n",
    "    similarities = model.similarity(query_embeddings, document_embeddings)\n",
    "    # Cleanup\n",
    "    del model\n",
    "    cleanup()\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "similarity_cache_path = \"data/cache/03_similarities.pt\"\n",
    "try:\n",
    "    similarities = torch.load(similarity_cache_path)\n",
    "except:\n",
    "    similarities = compute_similarities(benchmark, documents)\n",
    "    torch.save(similarities, similarity_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa7bddd-e445-4893-b073-542a568abc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSAMPLE = False\n",
    "\n",
    "if SUBSAMPLE:\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed(1996)\n",
    "    idxs = random.sample(range(len(benchmark)), 20)\n",
    "    \n",
    "    benchmark = [benchmark[idx] for idx in idxs]\n",
    "    similarities = similarities[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e218bafb-14d6-4a35-b9a8-6b6cb1f9b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = torch.argsort(similarities, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fe742f-1a2d-4013-bdc8-ee459d7dda90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dd6d0ec2ef4c1487a7201410a3f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def format_prompts(query: str, instruction: str, documents: list[str]) -> list[str]:\n",
    "    \"\"\"Format query and documents into prompts for reranking.\"\"\"\n",
    "    if instruction:\n",
    "        instruction = f\" {instruction}\"\n",
    "    prompts = []\n",
    "    for doc in documents:\n",
    "        prompt = f\"Check whether a given document contains information helpful to answer the query.\\n<Document> {doc}\\n<Query> {query}{instruction} ??\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "model_path = \"ContextualAI/ctxl-rerank-v2-instruct-multilingual-2b\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # so -1 is the real last token for all prompts\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "    dtype=dtype,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def infer_w_hf(model_path: str, query: str, instruction: str, documents: list[str]):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    prompts = format_prompts(query, instruction, documents)\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    next_logits = out.logits[:, -1, :]  # [batch, vocab]\n",
    "\n",
    "    scores_bf16 = next_logits[:, 0].to(torch.bfloat16)\n",
    "    scores = scores_bf16.float().tolist()\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    results = sorted([(s, i, documents[i]) for i, s in enumerate(scores)], key=lambda x: x[0], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08323fd2-a58d-441c-b35a-5fca7a01f3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbf49b3a19c49d0836a3f38f5746130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuachin/miniconda3/envs/RAG-2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "model_path = \"ContextualAI/ctxl-rerank-v2-instruct-multilingual-2b\"\n",
    "TOP_K = 32\n",
    "\n",
    "results = []\n",
    "for idx, (test, doc_idxs) in tqdm(\n",
    "    enumerate(zip(benchmark, ranks)),\n",
    "    total=min(len(benchmark), len(ranks))\n",
    "):\n",
    "    result = infer_w_hf(\n",
    "        model_path,\n",
    "        query=benchmark[idx]['query'],\n",
    "        instruction='',\n",
    "        documents= [f\"{documents[idx].metadata[\"source_file\"]}: {documents[idx].page_content}\" for idx in ranks[idx, :TOP_K]],\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "    if idx % 8 == 0:\n",
    "        cleanup()\n",
    "\n",
    "reranks = []\n",
    "for idx, result in enumerate(results):\n",
    "    top_documents = ranks[idx, :TOP_K]\n",
    "    base_document_idxs = [int(top_documents[relative_idx]) for score, relative_idx, content in result]\n",
    "    reranks.append(base_document_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b34ab1a-0b39-419d-8c23-8d83e612bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation\n",
      "precision @ 1 :  0.2450, recall @ 1 :  0.1492\n",
      "precision @ 2 :  0.2265, recall @ 2 :  0.2895\n",
      "precision @ 4 :  0.1758, recall @ 4 :  0.3731\n",
      "precision @ 8 :  0.1408, recall @ 8 :  0.5243\n",
      "precision @ 16:  0.1009, recall @ 16:  0.7081\n",
      "precision @ 32:  0.0685, recall @ 32:  0.8412\n",
      "precision @ 64:  0.0460, recall @ 64:  0.9538\n",
      "\n",
      "Reranked evaluation\n",
      "precision @ 1 :  0.3098, recall @ 1 :  0.1903\n",
      "precision @ 2 :  0.2659, recall @ 2 :  0.2901\n",
      "precision @ 4 :  0.2127, recall @ 4 :  0.4384\n",
      "precision @ 8 :  0.1548, recall @ 8 :  0.5639\n",
      "precision @ 16:  0.1100, recall @ 16:  0.7283\n",
      "precision @ 32:  0.0685, recall @ 32:  0.8412\n",
      "precision @ 64:  0.0685, recall @ 64:  0.8412\n"
     ]
    }
   ],
   "source": [
    "from lib.metrics import evaluate_rag, evaluate_rag_reranked\n",
    "\n",
    "print(\"Baseline evaluation\")\n",
    "for k in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    precision, recall = evaluate_rag(benchmark, documents, similarities, k)\n",
    "    print(f\"precision @ {k:<2}: {precision:7.4f}, recall @ {k:<2}: {recall:7.4f}\")\n",
    "\n",
    "print(\"\\nReranked evaluation\")\n",
    "for k in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    precision, recall = evaluate_rag_reranked(benchmark, documents, reranks, k)\n",
    "    print(f\"precision @ {k:<2}: {precision:7.4f}, recall @ {k:<2}: {recall:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c361b-99c1-4201-9ca0-a8c178434214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
