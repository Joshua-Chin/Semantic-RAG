{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b74260-3bb3-4e2c-a671-44bb0c0c4eff",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0956bb86-2d27-465d-a9a9-76dda1947b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib.load import extract_data, load_benchmark_corpus\n",
    "\n",
    "\n",
    "extract_data()\n",
    "benchmark, corpus = load_benchmark_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bd70f-886f-4fbf-beb6-1e7015c75f21",
   "metadata": {},
   "source": [
    "## Split Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2929d1d0-3cbd-4410-b398-153f3496f312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source_file': 'Fiverr.txt', 'start_index': 2}, page_content='At Fiverr we care about your privacy.\\nWe do not sell or rent your personal information to third parties for their direct marketing purposes without your explicit consent.'),\n",
       " Document(metadata={'source_file': 'Fiverr.txt', 'start_index': 173}, page_content='We do not disclose it to others except as disclosed in this Policy or required to provide you with the services of the Site and mobile applications, meaning - to allow you to buy, sell, share the information you want to share on the Site; to contribute on the forum; pay for products; post reviews and so on; or where we have a legal obligation to do so.'),\n",
       " Document(metadata={'source_file': 'Fiverr.txt', 'start_index': 530}, page_content='We collect information that you provide us or voluntarily share with other users, and also some general technical information that is automatically gathered by our systems, such as IP address, browser information and cookies to enable you to have a better user experience and a more personalized browsing experience.\\n  We will not share information that you provide us in the process of the registration - including your contact information - except as described in this Policy.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n', '!', '?', '.', ':', ';', ',', ' ', ''],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "names, texts = zip(*corpus.items())\n",
    "metadatas = [\n",
    "    {\"source_file\": name}\n",
    "    for idx, name in enumerate(names)\n",
    "]\n",
    "\n",
    "documents = text_splitter.create_documents(corpus.values(), metadatas=metadatas)\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567497ed-7916-4097-9d41-903e5e4ee0f5",
   "metadata": {},
   "source": [
    "## Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9b6f5-db2f-4868-a294-70ac015f9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def compute_similarities(benchmark, documents):\n",
    "    # Load model\n",
    "    model = SentenceTransformer(\n",
    "        \"Qwen/Qwen3-Embedding-8B\",\n",
    "        model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)}\n",
    "    )\n",
    "    # Compute embeddings\n",
    "    document_embeddings = model.encode(\n",
    "        [f\"{document.metadata[\"source_file\"]}: {document.page_content}\" for document in documents],\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    query_embeddings = model.encode(\n",
    "        [test['query'] for test in benchmark],\n",
    "        prompt_name=\"query\",\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    # Compute similarity\n",
    "    similarities = model.similarity(query_embeddings, document_embeddings)\n",
    "    # Cleanup\n",
    "    del model\n",
    "    cleanup()\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if True:\n",
    "    similarities = torch.load(\"sim_cache\")\n",
    "else:\n",
    "    similarities = compute_similarities(benchmark, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa7bddd-e445-4893-b073-542a568abc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1996)\n",
    "idxs = random.sample(range(len(benchmark)), 20)\n",
    "\n",
    "benchmark = [benchmark[idx] for idx in idxs]\n",
    "similarities = similarities[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fe742f-1a2d-4013-bdc8-ee459d7dda90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f513779cabb46d1b01303d9358bb12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/joshuachin/miniconda3/envs/RAG-2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def rerank(queries, documents, scores, topk=32, batchsize=32):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-8B\", padding_side='left')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Qwen/Qwen3-Reranker-0.6B\",\n",
    "        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    ).eval()\n",
    "\n",
    "\n",
    "    pairs = []\n",
    "    extras = []\n",
    "    for idx, (query, document_idxs) in enumerate(zip(queries, torch.argsort(scores, descending=True)[:, :topk])):\n",
    "        pairs += [(query, documents[document_idx]) for document_idx in document_idxs]\n",
    "        extras += [(idx, document_idx) for document_idx in document_idxs]\n",
    "\n",
    "    new_scores = []\n",
    "    new_extras = []\n",
    "    for i in trange(0, len(pairs), batchsize):    \n",
    "        new_scores += rescore_qwen(tokenizer, model, pairs[i:i+batchsize])\n",
    "        new_extras += extras[i:i+batchsize]\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return new_scores, new_extras\n",
    "    \n",
    "\n",
    "def rescore_qwen(tokenizer, model, pairs):\n",
    "    def format_instruction(instruction, query, doc):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "        return output\n",
    "    \n",
    "    def process_inputs(pairs):\n",
    "        inputs = tokenizer(\n",
    "            pairs, padding=False, truncation='longest_first',\n",
    "            return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "        )\n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "        inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(model.device)\n",
    "        return inputs\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute_logits(inputs, **kwargs):\n",
    "        batch_scores = model(**inputs).logits[:, -1, :]\n",
    "        true_vector = batch_scores[:, token_true_id]\n",
    "        false_vector = batch_scores[:, token_false_id]\n",
    "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "        scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "    \n",
    "    token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "    token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "    max_length = 8192\n",
    "    \n",
    "    prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "            \n",
    "    task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    \n",
    "    pairs = [format_instruction(task, query, doc) for query, doc in pairs]\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    inputs = process_inputs(pairs)\n",
    "    scores = compute_logits(inputs)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "new_score, new_extras = rerank(\n",
    "    [test['query'] for test in benchmark],\n",
    "    [f\"{document.metadata[\"source_file\"]}: {document.page_content}\" for document in documents],\n",
    "    similarities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb29f32-3548-4339-a259-10f6d5f6201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9990234375, np.int64(188)), (0.99853515625, np.int64(233)), (0.99853515625, np.int64(187)), (0.998046875, np.int64(225)), (0.998046875, np.int64(192)), (0.99755859375, np.int64(250)), (0.99755859375, np.int64(218)), (0.99755859375, np.int64(191)), (0.99755859375, np.int64(189)), (0.9970703125, np.int64(258)), (0.9970703125, np.int64(240)), (0.9970703125, np.int64(222)), (0.99658203125, np.int64(249)), (0.99658203125, np.int64(174)), (0.99609375, np.int64(176)), (0.99560546875, np.int64(267)), (0.99560546875, np.int64(256)), (0.99560546875, np.int64(251)), (0.9951171875, np.int64(186)), (0.994140625, np.int64(273)), (0.99365234375, np.int64(163)), (0.9931640625, np.int64(184)), (0.99169921875, np.int64(223)), (0.9912109375, np.int64(195)), (0.98876953125, np.int64(190)), (0.98681640625, np.int64(246)), (0.98583984375, np.int64(169)), (0.9853515625, np.int64(175)), (0.984375, np.int64(198)), (0.97314453125, np.int64(199)), (0.96435546875, np.int64(211)), (0.93310546875, np.int64(161))]\n"
     ]
    }
   ],
   "source": [
    "y = new_score\n",
    "document_idxs = torch.argsort(similarities, descending=True)[:, :32].numpy()\n",
    "\n",
    "blocks = []\n",
    "for i in range(0, len(y), 64):\n",
    "    block = list(zip(y[i:i+64], document_idxs[i // 64]))\n",
    "    block.sort(reverse=True)\n",
    "    if i == 0:\n",
    "        print(block)\n",
    "    blocks.append([int(idx) for _, idx in block])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90017372-2f8b-42c4-b3a1-6fb851c06777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.metrics import precision_recall\n",
    "\n",
    "def evaluate_rag(benchmark, documents, similarities, topk):\n",
    "    document_idxs_by_rank = torch.argsort(similarities, descending=True)[:, :topk]\n",
    "    return evaluate_rag_reranked(benchmark, documents, document_idxs_by_rank, topk)\n",
    "\n",
    "\n",
    "def evaluate_rag_reranked(benchmark, documents, document_idxs_by_rank, topk):\n",
    "    precision = recall = 0\n",
    "    count = 0\n",
    "    for test, document_idxs in zip(benchmark, document_idxs_by_rank):\n",
    "        document_idxs = document_idxs[:topk]\n",
    "        # Compute spans\n",
    "        spans_true = []\n",
    "        for snippet in test[\"snippets\"]:\n",
    "            spans_true.append(snippet[\"span\"])\n",
    "        spans_pred = []\n",
    "        for idx in document_idxs:\n",
    "            document = documents[idx]\n",
    "            start = document.metadata[\"start_index\"]\n",
    "            length = len(document.page_content)\n",
    "            spans_pred.append((start, start + length))\n",
    "        # Compute precision and recall\n",
    "        p, r = precision_recall(spans_true, spans_pred)\n",
    "        # Update accumulators\n",
    "        precision += p\n",
    "        recall += r\n",
    "        count += 1\n",
    "    return precision / count, recall / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4474c01-a8e5-4d1b-8d19-94c6d472ff75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24589244289064832, 0.3285857284815846)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rag_reranked(benchmark, documents, torch.argsort(similarities, descending=True), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fefed5df-6fb2-4d2f-85c8-3d870dda4609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1364112818056034, 0.09770221601774717)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rag_reranked(benchmark, documents, blocks, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6730af-87af-4dee-9814-447818e60036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[188,\n",
       " 233,\n",
       " 187,\n",
       " 225,\n",
       " 192,\n",
       " 250,\n",
       " 218,\n",
       " 191,\n",
       " 189,\n",
       " 258,\n",
       " 240,\n",
       " 222,\n",
       " 249,\n",
       " 174,\n",
       " 176,\n",
       " 267,\n",
       " 256,\n",
       " 251,\n",
       " 186,\n",
       " 273,\n",
       " 163,\n",
       " 184,\n",
       " 223,\n",
       " 195,\n",
       " 190,\n",
       " 246,\n",
       " 169,\n",
       " 175,\n",
       " 198,\n",
       " 199,\n",
       " 211,\n",
       " 161]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb335e3-8ecc-4c70-bc11-ca770e43693e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(torch.argsort(similarities, descending=True)[0].tolist()) & set(blocks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881b14b-36df-47cc-83c8-015c8dec9f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
