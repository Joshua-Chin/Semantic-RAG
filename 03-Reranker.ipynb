{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b74260-3bb3-4e2c-a671-44bb0c0c4eff",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0956bb86-2d27-465d-a9a9-76dda1947b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rag.load import load_benchmark_corpus, corpus_to_texts_metadatas\n",
    "\n",
    "\n",
    "benchmark, corpus = load_benchmark_corpus()\n",
    "texts, metadatas = corpus_to_texts_metadatas(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bd70f-886f-4fbf-beb6-1e7015c75f21",
   "metadata": {},
   "source": [
    "## Split Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2929d1d0-3cbd-4410-b398-153f3496f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n', '!', '?', '.', ':', ';', ',', ' ', ''],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents(texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567497ed-7916-4097-9d41-903e5e4ee0f5",
   "metadata": {},
   "source": [
    "## Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9b6f5-db2f-4868-a294-70ac015f9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from rag.embed import compute_similarities, get_query_strings, get_document_contents\n",
    "from rag.util import cleanup\n",
    "\n",
    "similarity_cache_path = \"data/cache/03_similarities.pt\"\n",
    "try:\n",
    "    similarities = torch.load(similarity_cache_path)\n",
    "except:\n",
    "    similarities = compute_similarities(\n",
    "        \"Qwen/Qwen3-Embedding-8B\",\n",
    "        queries=get_query_strings(benchmark),\n",
    "        documents=get_document_contents(documents),\n",
    "    )\n",
    "    torch.save(similarities, similarity_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7fe742f-1a2d-4013-bdc8-ee459d7dda90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from rag.util import cleanup\n",
    "\n",
    "\n",
    "def format_prompts(query: str, instruction: str, documents: list[str]) -> list[str]:\n",
    "    \"\"\"Format query and documents into prompts for reranking.\"\"\"\n",
    "    if instruction:\n",
    "        instruction = f\" {instruction}\"\n",
    "    prompts = []\n",
    "    for doc in documents:\n",
    "        prompt = f\"Check whether a given document contains information helpful to answer the query.\\n<Document> {doc}\\n<Query> {query}{instruction} ??\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def infer_w_hf(model, tokenizer, query: str, instruction: str, documents: list[str]):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    prompts = format_prompts(query, instruction, documents)\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    cleanup()\n",
    "\n",
    "    next_logits = out.logits[:, -1, :]  # [batch, vocab]\n",
    "\n",
    "    scores_bf16 = next_logits[:, 0].to(torch.bfloat16)\n",
    "    scores = scores_bf16.float().tolist()\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    results = sorted([(s, i, documents[i]) for i, s in enumerate(scores)], key=lambda x: x[0], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08323fd2-a58d-441c-b35a-5fca7a01f3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1c31920be64eeab2d7877563c4f640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0318c4e3ed48b395f9a29c223e7c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuachin/miniconda3/envs/RAG-2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from rag.metrics import similarities_to_ranks\n",
    "from rag.embed import get_query_strings, get_document_contents\n",
    "\n",
    "\n",
    "TOP_K = 32\n",
    "model_path = \"ContextualAI/ctxl-rerank-v2-instruct-multilingual-2b\"\n",
    "\n",
    "# Load the reranker\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # so -1 is the real last token for all prompts\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "    dtype=dtype,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Run the evaluation\n",
    "ranks = similarities_to_ranks(similarities)\n",
    "results = []\n",
    "for idx, (test, doc_idxs) in tqdm(\n",
    "    enumerate(zip(benchmark, ranks)),\n",
    "    total=min(len(benchmark), len(ranks))\n",
    "):\n",
    "    top_documents = [documents[doc_idx] for doc_idx in doc_idxs[:TOP_K]]\n",
    "    result = infer_w_hf(\n",
    "        model, tokenizer,\n",
    "        query=get_query_strings([benchmark[idx]])[0],\n",
    "        instruction='',\n",
    "        documents=get_document_contents(top_documents),\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "reranks = []\n",
    "for idx, result in enumerate(results):\n",
    "    top_documents = ranks[idx, :TOP_K]\n",
    "    base_document_idxs = [int(top_documents[relative_idx]) for score, relative_idx, content in result]\n",
    "    reranks.append(base_document_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b34ab1a-0b39-419d-8c23-8d83e612bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation\n",
      "precision @ 1 :  0.2450, recall @ 1 :  0.1492\n",
      "precision @ 2 :  0.2265, recall @ 2 :  0.2895\n",
      "precision @ 4 :  0.1758, recall @ 4 :  0.3731\n",
      "precision @ 8 :  0.1408, recall @ 8 :  0.5243\n",
      "precision @ 16:  0.1009, recall @ 16:  0.7081\n",
      "precision @ 32:  0.0685, recall @ 32:  0.8412\n",
      "precision @ 64:  0.0460, recall @ 64:  0.9538\n",
      "AUC: 0.10639778453742046\n",
      "\n",
      "Reranked evaluation\n",
      "precision @ 1 :  0.3098, recall @ 1 :  0.1903\n",
      "precision @ 2 :  0.2659, recall @ 2 :  0.2901\n",
      "precision @ 4 :  0.2127, recall @ 4 :  0.4384\n",
      "precision @ 8 :  0.1548, recall @ 8 :  0.5639\n",
      "precision @ 16:  0.1100, recall @ 16:  0.7283\n",
      "precision @ 32:  0.0685, recall @ 32:  0.8412\n",
      "precision @ 64:  0.0685, recall @ 64:  0.8412\n",
      "AUC: 0.12047913911889209\n"
     ]
    }
   ],
   "source": [
    "from rag.metrics import print_evaluations\n",
    "\n",
    "print(\"Baseline evaluation\")\n",
    "print_evaluations(benchmark, documents, ranks)\n",
    "\n",
    "print(\"\\nReranked evaluation\")\n",
    "print_evaluations(benchmark, documents, reranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c361b-99c1-4201-9ca0-a8c178434214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
