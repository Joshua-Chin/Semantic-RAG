{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b74260-3bb3-4e2c-a671-44bb0c0c4eff",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0956bb86-2d27-465d-a9a9-76dda1947b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from lib.load import extract_data, LEGALBENCH_RAG_PATH\n",
    "\n",
    "\n",
    "extract_data()\n",
    "\n",
    "def load_benchmark_corpus(subset=\"maud\"):\n",
    "    with open(os.path.join(LEGALBENCH_RAG_PATH, \"benchmarks\", f\"{subset}.json\")) as f:\n",
    "        benchmark = json.load(f)['tests']\n",
    "    \n",
    "    corpus = {}\n",
    "    corpus_path = os.path.join(LEGALBENCH_RAG_PATH, \"corpus\", subset)\n",
    "\n",
    "    random.seed(42)\n",
    "    \n",
    "    for document in random.sample(os.listdir(corpus_path), 5):\n",
    "        with open(os.path.join(corpus_path, document)) as f:\n",
    "            corpus[document] = f.read()\n",
    "\n",
    "    benchmark_sample = []\n",
    "    for test in benchmark:\n",
    "        file_path = test[\"snippets\"][0][\"file_path\"]\n",
    "        filename = os.path.basename(file_path)\n",
    "        if filename in corpus:\n",
    "            benchmark_sample.append(test)\n",
    "            \n",
    "    return benchmark_sample, corpus\n",
    "benchmark, corpus = load_benchmark_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bd70f-886f-4fbf-beb6-1e7015c75f21",
   "metadata": {},
   "source": [
    "## Split Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2929d1d0-3cbd-4410-b398-153f3496f312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source_file': 'Domtar Corporation_Paper Excellence Canada Group.txt', 'start_index': 0}, page_content='\\ufeffExhibit 2.1 \\n\\n\\nExecution Version     AGREEMENT AND PLAN OF MERGER \\n\\n\\namong \\n\\n\\nDOMTAR CORPORATION, \\n\\n\\nKARTA HALTEN B.V., \\n\\n\\nand \\n\\n\\nPEARL MERGER SUB INC. \\n\\n\\nand \\n\\n\\nPAPER EXCELLENCE B.V. \\n\\n\\nand \\n\\n\\nHERVEY INVESTMENTS B.V. \\n\\n\\nDated as of May 10, 2021    \\n\\n\\n\\n\\n\\n\\n\\n\\n________________'),\n",
       " Document(metadata={'source_file': 'Domtar Corporation_Paper Excellence Canada Group.txt', 'start_index': 278}, page_content='TABLE OF CONTENTS         Page  ARTICLE I    DEFINITIONS    Section 1.1   Definitions    6  Section 1.2   Table of Definitions    20  Section 1.3   Other Definitional and Interpretative Provisions    22  ARTICLE II    THE MERGER; EFFECT ON THE CAPITAL STOCK; PAYMENT    Section 2.1   The Merger    23  Section 2.2   Closing    23  Section 2.3   Effective Time    23  Section 2.4   Surviving Corporation Matters    24  Section 2'),\n",
       " Document(metadata={'source_file': 'Domtar Corporation_Paper Excellence Canada Group.txt', 'start_index': 705}, page_content='.5   Effect of the Merger on Capital Stock of the Company and Merger Sub    24  Section 2.6   Certain Adjustments    25  Section 2.7   Appraisal Shares    25  Section 2.8   Payment for Company Stock    25  Section 2.9   Further Assurances    28  Section 2.10  Treatment of Company Awards    28  Section 2.11  Withholding    30  Section 2.12  Escrow Deposit and Release    30  Section 2.13  FIRPTA Certificate    30  ARTICLE III    REPRESENTATIONS AND WARRANTIES OF THE COMPANY    Section 3')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n', '!', '?', '.', ':', ';', ',', ' ', ''],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "names, texts = zip(*corpus.items())\n",
    "metadatas = [\n",
    "    {\"source_file\": name}\n",
    "    for idx, name in enumerate(names)\n",
    "]\n",
    "\n",
    "documents = text_splitter.create_documents(corpus.values(), metadatas=metadatas)\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567497ed-7916-4097-9d41-903e5e4ee0f5",
   "metadata": {},
   "source": [
    "## Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9b6f5-db2f-4868-a294-70ac015f9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def compute_similarities(benchmark, documents):\n",
    "    # Load model\n",
    "    model = SentenceTransformer(\n",
    "        \"Qwen/Qwen3-Embedding-8B\",\n",
    "        model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)}\n",
    "    )\n",
    "    # Compute embeddings\n",
    "    document_embeddings = model.encode(\n",
    "        [f\"{document.metadata[\"source_file\"]}: {document.page_content}\" for document in documents],\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    query_embeddings = model.encode(\n",
    "        [test['query'] for test in benchmark],\n",
    "        prompt_name=\"query\",\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    # Compute similarity\n",
    "    similarities = model.similarity(query_embeddings, document_embeddings)\n",
    "    # Cleanup\n",
    "    del model\n",
    "    cleanup()\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "similarity_cache_path = \"data/cache/04_similarities_maud.pt\"\n",
    "try:\n",
    "    similarities = torch.load(similarity_cache_path)\n",
    "except:\n",
    "    similarities = compute_similarities(benchmark, documents)\n",
    "    torch.save(similarities, similarity_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e218bafb-14d6-4a35-b9a8-6b6cb1f9b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = torch.argsort(similarities, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08323fd2-a58d-441c-b35a-5fca7a01f3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671672120287476a866d98b1e5b8388e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5cfa2f5d7042bf82cc9703bd55eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuachin/miniconda3/envs/RAG-2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lib.rerank import Reranker\n",
    "\n",
    "\n",
    "model_path = \"ContextualAI/ctxl-rerank-v2-instruct-multilingual-2b\"\n",
    "TOP_K = 32\n",
    "\n",
    "def rerank(benchmark, documents, ranks, model_path=model_path, topk=TOP_K):\n",
    "    reranker = Reranker(model_path)\n",
    "    \n",
    "    results = []\n",
    "    for idx, (test, doc_idxs) in tqdm(\n",
    "        enumerate(zip(benchmark, ranks)),\n",
    "        total=min(len(benchmark), len(ranks))\n",
    "    ):\n",
    "        result = reranker(\n",
    "            query=benchmark[idx]['query'],\n",
    "            instruction='',\n",
    "            documents=[\n",
    "                f\"{documents[document_idx].metadata[\"source_file\"]}: {documents[document_idx].page_content}\" \n",
    "                for document_idx in ranks[idx, :TOP_K]\n",
    "            ],\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "        if idx % 8 == 0:\n",
    "            cleanup()\n",
    "    \n",
    "    reranks = []\n",
    "    for idx, result in enumerate(results):\n",
    "        top_documents = ranks[idx, :TOP_K]\n",
    "        base_document_idxs = [int(top_documents[relative_idx]) for score, relative_idx, content in result]\n",
    "        reranks.append(base_document_idxs)\n",
    "    return reranks\n",
    "\n",
    "reranks = rerank(benchmark, documents, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b34ab1a-0b39-419d-8c23-8d83e612bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Evaluation\n",
      "precision @ 1 :  0.1935, recall @ 1 :  0.0918\n",
      "precision @ 2 :  0.1687, recall @ 2 :  0.1278\n",
      "precision @ 4 :  0.1368, recall @ 4 :  0.2127\n",
      "precision @ 8 :  0.1173, recall @ 8 :  0.3382\n",
      "precision @ 16:  0.0766, recall @ 16:  0.4198\n",
      "precision @ 32:  0.0522, recall @ 32:  0.5222\n",
      "precision @ 64:  0.0352, recall @ 64:  0.6287\n",
      "AUC: 0.05023092285649827\n",
      "\n",
      "Reranked evaluation\n",
      "precision @ 1 :  0.2991, recall @ 1 :  0.1622\n",
      "precision @ 2 :  0.1922, recall @ 2 :  0.1920\n",
      "precision @ 4 :  0.1533, recall @ 4 :  0.2744\n",
      "precision @ 8 :  0.1145, recall @ 8 :  0.3628\n",
      "precision @ 16:  0.0754, recall @ 16:  0.4222\n",
      "precision @ 32:  0.0522, recall @ 32:  0.5222\n",
      "precision @ 64:  0.0522, recall @ 64:  0.5222\n",
      "AUC: 0.06668000103732792\n"
     ]
    }
   ],
   "source": [
    "from lib.metrics import print_evaluations\n",
    "\n",
    "print(\"Baseline Evaluation\")\n",
    "print_evaluations(benchmark, documents, ranks)\n",
    "\n",
    "print(\"\\nReranked evaluation\")\n",
    "print_evaluations(benchmark, documents, reranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe470116-8bdd-46d8-ac9a-3e3ac28e2de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
