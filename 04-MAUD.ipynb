{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b74260-3bb3-4e2c-a671-44bb0c0c4eff",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0956bb86-2d27-465d-a9a9-76dda1947b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from lib.load import extract_data, LEGALBENCH_RAG_PATH\n",
    "\n",
    "\n",
    "extract_data()\n",
    "\n",
    "def load_benchmark_corpus(subset=\"maud\"):\n",
    "    with open(os.path.join(LEGALBENCH_RAG_PATH, \"benchmarks\", f\"{subset}.json\")) as f:\n",
    "        benchmark = json.load(f)['tests']\n",
    "    \n",
    "    corpus = {}\n",
    "    corpus_path = os.path.join(LEGALBENCH_RAG_PATH, \"corpus\", subset)\n",
    "\n",
    "    random.seed(42)\n",
    "    \n",
    "    for document in random.sample(os.listdir(corpus_path), 5):\n",
    "        with open(os.path.join(corpus_path, document)) as f:\n",
    "            corpus[document] = f.read()\n",
    "\n",
    "    benchmark_sample = []\n",
    "    for test in benchmark:\n",
    "        file_path = test[\"snippets\"][0][\"file_path\"]\n",
    "        filename = os.path.basename(file_path)\n",
    "        if filename in corpus:\n",
    "            benchmark_sample.append(test)\n",
    "            \n",
    "    return benchmark_sample, corpus\n",
    "benchmark, corpus = load_benchmark_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bd70f-886f-4fbf-beb6-1e7015c75f21",
   "metadata": {},
   "source": [
    "## Split Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2929d1d0-3cbd-4410-b398-153f3496f312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source_file': 'Domtar Corporation_Paper Excellence Canada Group.txt', 'start_index': 0}, page_content='\\ufeffExhibit 2.1 \\n\\n\\nExecution Version     AGREEMENT AND PLAN OF MERGER \\n\\n\\namong \\n\\n\\nDOMTAR CORPORATION, \\n\\n\\nKARTA HALTEN B.V., \\n\\n\\nand \\n\\n\\nPEARL MERGER SUB INC. \\n\\n\\nand \\n\\n\\nPAPER EXCELLENCE B.V. \\n\\n\\nand \\n\\n\\nHERVEY INVESTMENTS B.V. \\n\\n\\nDated as of May 10, 2021    \\n\\n\\n\\n\\n\\n\\n\\n\\n________________'),\n",
       " Document(metadata={'source_file': 'Domtar Corporation_Paper Excellence Canada Group.txt', 'start_index': 278}, page_content='TABLE OF CONTENTS         Page  ARTICLE I    DEFINITIONS    Section 1.1   Definitions    6  Section 1.2   Table of Definitions    20  Section 1.3   Other Definitional and Interpretative Provisions    22  ARTICLE II    THE MERGER; EFFECT ON THE CAPITAL STOCK; PAYMENT    Section 2.1   The Merger    23  Section 2.2   Closing    23  Section 2.3   Effective Time    23  Section 2.4   Surviving Corporation Matters    24  Section 2'),\n",
       " Document(metadata={'source_file': 'Domtar Corporation_Paper Excellence Canada Group.txt', 'start_index': 705}, page_content='.5   Effect of the Merger on Capital Stock of the Company and Merger Sub    24  Section 2.6   Certain Adjustments    25  Section 2.7   Appraisal Shares    25  Section 2.8   Payment for Company Stock    25  Section 2.9   Further Assurances    28  Section 2.10  Treatment of Company Awards    28  Section 2.11  Withholding    30  Section 2.12  Escrow Deposit and Release    30  Section 2.13  FIRPTA Certificate    30  ARTICLE III    REPRESENTATIONS AND WARRANTIES OF THE COMPANY    Section 3')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n', '\\n', '!', '?', '.', ':', ';', ',', ' ', ''],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "names, texts = zip(*corpus.items())\n",
    "metadatas = [\n",
    "    {\"source_file\": name}\n",
    "    for idx, name in enumerate(names)\n",
    "]\n",
    "\n",
    "documents = text_splitter.create_documents(corpus.values(), metadatas=metadatas)\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567497ed-7916-4097-9d41-903e5e4ee0f5",
   "metadata": {},
   "source": [
    "## Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b9b6f5-db2f-4868-a294-70ac015f9320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6e7f3b35694f1ca822c50a07bae2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0559ab41c0ff465d898dfb520bfb5207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b47ac1d623e458bb2f3375371210233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def compute_similarities(benchmark, documents):\n",
    "    # Load model\n",
    "    model = SentenceTransformer(\n",
    "        \"Qwen/Qwen3-Embedding-8B\",\n",
    "        model_kwargs={\"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)}\n",
    "    )\n",
    "    # Compute embeddings\n",
    "    document_embeddings = model.encode(\n",
    "        [f\"{document.metadata[\"source_file\"]}: {document.page_content}\" for document in documents],\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    query_embeddings = model.encode(\n",
    "        [test['query'] for test in benchmark],\n",
    "        prompt_name=\"query\",\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    # Compute similarity\n",
    "    similarities = model.similarity(query_embeddings, document_embeddings)\n",
    "    # Cleanup\n",
    "    del model\n",
    "    cleanup()\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "similarity_cache_path = \"data/cache/04_similarities_maud.pt\"\n",
    "try:\n",
    "    similarities = torch.load(similarity_cache_path)\n",
    "except:\n",
    "    similarities = compute_similarities(benchmark, documents)\n",
    "    torch.save(similarities, similarity_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "249fc74e-6272-4f7e-a5fb-8ae08a7be92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation\n",
      "precision @ 1 :  0.1935, recall @ 1 :  0.0918\n",
      "precision @ 2 :  0.1687, recall @ 2 :  0.1278\n",
      "precision @ 4 :  0.1368, recall @ 4 :  0.2127\n",
      "precision @ 8 :  0.1173, recall @ 8 :  0.3382\n",
      "precision @ 16:  0.0766, recall @ 16:  0.4198\n",
      "precision @ 32:  0.0522, recall @ 32:  0.5222\n",
      "precision @ 64:  0.0352, recall @ 64:  0.6287\n"
     ]
    }
   ],
   "source": [
    "from lib.metrics import evaluate_rag\n",
    "\n",
    "print(\"Baseline evaluation\")\n",
    "for k in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    precision, recall = evaluate_rag(benchmark, documents, similarities, k)\n",
    "    print(f\"precision @ {k:<2}: {precision:7.4f}, recall @ {k:<2}: {recall:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e218bafb-14d6-4a35-b9a8-6b6cb1f9b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = torch.argsort(similarities, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7fe742f-1a2d-4013-bdc8-ee459d7dda90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f575d75864dc48e5b8525e59c27d17de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def format_prompts(query: str, instruction: str, documents: list[str]) -> list[str]:\n",
    "    \"\"\"Format query and documents into prompts for reranking.\"\"\"\n",
    "    if instruction:\n",
    "        instruction = f\" {instruction}\"\n",
    "    prompts = []\n",
    "    for doc in documents:\n",
    "        prompt = f\"Check whether a given document contains information helpful to answer the query.\\n<Document> {doc}\\n<Query> {query}{instruction} ??\"\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "model_path = \"ContextualAI/ctxl-rerank-v2-instruct-multilingual-2b\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # so -1 is the real last token for all prompts\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "    dtype=dtype,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def infer_w_hf(model_path: str, query: str, instruction: str, documents: list[str]):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    prompts = format_prompts(query, instruction, documents)\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    next_logits = out.logits[:, -1, :]  # [batch, vocab]\n",
    "\n",
    "    scores_bf16 = next_logits[:, 0].to(torch.bfloat16)\n",
    "    scores = scores_bf16.float().tolist()\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    results = sorted([(s, i, documents[i]) for i, s in enumerate(scores)], key=lambda x: x[0], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08323fd2-a58d-441c-b35a-5fca7a01f3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6269cf8587674edb8ed7ae40b92e2418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuachin/miniconda3/envs/RAG-2/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "model_path = \"ContextualAI/ctxl-rerank-v2-instruct-multilingual-2b\"\n",
    "TOP_K = 32\n",
    "\n",
    "results = []\n",
    "for idx, (test, doc_idxs) in tqdm(\n",
    "    enumerate(zip(benchmark, ranks)),\n",
    "    total=min(len(benchmark), len(ranks))\n",
    "):\n",
    "    result = infer_w_hf(\n",
    "        model_path,\n",
    "        query=benchmark[idx]['query'],\n",
    "        instruction='',\n",
    "        documents= [f\"{documents[idx].metadata[\"source_file\"]}: {documents[idx].page_content}\" for idx in ranks[idx, :TOP_K]],\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "    if idx % 8 == 0:\n",
    "        cleanup()\n",
    "\n",
    "reranks = []\n",
    "for idx, result in enumerate(results):\n",
    "    top_documents = ranks[idx, :TOP_K]\n",
    "    base_document_idxs = [int(top_documents[relative_idx]) for score, relative_idx, content in result]\n",
    "    reranks.append(base_document_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b34ab1a-0b39-419d-8c23-8d83e612bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation\n",
      "precision @ 1 :  0.1935, recall @ 1 :  0.0918\n",
      "precision @ 2 :  0.1687, recall @ 2 :  0.1278\n",
      "precision @ 4 :  0.1368, recall @ 4 :  0.2127\n",
      "precision @ 8 :  0.1173, recall @ 8 :  0.3382\n",
      "precision @ 16:  0.0766, recall @ 16:  0.4198\n",
      "precision @ 32:  0.0522, recall @ 32:  0.5222\n",
      "\n",
      "Reranked evaluation\n",
      "precision @ 1 :  0.2991, recall @ 1 :  0.1622\n",
      "precision @ 2 :  0.1922, recall @ 2 :  0.1920\n",
      "precision @ 4 :  0.1533, recall @ 4 :  0.2744\n",
      "precision @ 8 :  0.1145, recall @ 8 :  0.3628\n",
      "precision @ 16:  0.0754, recall @ 16:  0.4222\n",
      "precision @ 32:  0.0522, recall @ 32:  0.5222\n"
     ]
    }
   ],
   "source": [
    "from lib.metrics import evaluate_rag, evaluate_rag_reranked\n",
    "\n",
    "print(\"Baseline evaluation\")\n",
    "for k in [1, 2, 4, 8, 16, 32]:\n",
    "    precision, recall = evaluate_rag(benchmark, documents, similarities, k)\n",
    "    print(f\"precision @ {k:<2}: {precision:7.4f}, recall @ {k:<2}: {recall:7.4f}\")\n",
    "\n",
    "print(\"\\nReranked evaluation\")\n",
    "for k in [1, 2, 4, 8, 16, 32]:\n",
    "    precision, recall = evaluate_rag_reranked(benchmark, documents, reranks, k)\n",
    "    print(f\"precision @ {k:<2}: {precision:7.4f}, recall @ {k:<2}: {recall:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a243df-42f9-407b-a542-e102cdfe2ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
